{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296422 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "sgyol ge pesx iapmyskni iaxe osxdhoropebabwacrslevthr  vatd apzershw iosfbh tu t\n",
      "vkikusrtinlaplgxaaesahuviir cfumkvb lffhcbsnm hhnxreinpnwcexcrtu eq nq ifye  cgh\n",
      "ue oacifamlaanrrmrgsgt qtaorpanmrvyzdaleehif ydappdovbrsupdextkeh nlckx tynthir \n",
      "wtyhkci hva edpaempnu f epngop hsuerc b xvuplzol eltjmgieb rsdrfmnaabhgrnsxnevqo\n",
      "wirrtpznpu dtxijediviyfrsw     djnxoijuzdssshnaeagcxedw hol ochvrr  stskwetais  \n",
      "================================================================================\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 100: 2.597107 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.95\n",
      "Validation set perplexity: 10.50\n",
      "Average loss at step 200: 2.250847 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.61\n",
      "Validation set perplexity: 8.72\n",
      "Average loss at step 300: 2.095811 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 400: 1.995004 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 500: 1.933710 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 600: 1.906480 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 700: 1.860627 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 800: 1.818159 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 900: 1.826402 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1000: 1.825814 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "quatical sime the wition one tio oree presters and disme as reporsian and ploso \n",
      "an inccedel subncdeff refiny and kind are not oreatohic in evee therah rustical \n",
      "ost praveliot of an filp beecen hicanions watially of be and s cltbiples grazici\n",
      "an dicolly dicy moviv of exoves beatle und excomprovivimicare infocies of shist \n",
      "verolivation in the gas caraly bimo s and froven five hove one carsed curprabect\n",
      "================================================================================\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1100: 1.777751 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1200: 1.755243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1300: 1.733042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1400: 1.746683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1500: 1.735738 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1600: 1.747109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1700: 1.711589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1800: 1.673248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1900: 1.647601 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2000: 1.691090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "jeung world natator geapu if futtings as a m spate vasting longillyasing one eig\n",
      "extubus in the boxker charartish evided beroilgy maecmically side praduped the e\n",
      "vanding works and signes to influeft exilaul wordd assonall dividian procialter \n",
      "x camitady from throon of themen everated the gaider of and infinite power zed t\n",
      "forsa emperor mas iston and persced of pownicing and puriting jownish but nove r\n",
      "================================================================================\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2100: 1.683619 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2200: 1.678720 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2300: 1.637295 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2400: 1.658749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2500: 1.677516 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2600: 1.649639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2700: 1.655868 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2800: 1.648499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2900: 1.645187 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3000: 1.649802 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "zed it call a that to chanicaly basing with a firm in their ladqus inform raftis\n",
      "qy words and sponitol incollew usetiture technitation of the generally the aulit\n",
      "posic arey the gaodi fear even the out foulds is origined army yearse procivers \n",
      "g nearly the ras s simples credies a datterdiygarism is the rakes were a cald fr\n",
      "blishts of one nine eight five air cant sawwanthension one eight six that is fiv\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3100: 1.624346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3200: 1.640842 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3300: 1.634420 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3400: 1.666997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3500: 1.654573 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3600: 1.665751 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3700: 1.643917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3800: 1.643406 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3900: 1.634327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.650159 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "ing and the later forces newowger with become the tassic indo the didurly he d t\n",
      "rowndard such one three binys and liberiftes live in dejegiar in pats fol govere\n",
      "es whicher bew more whese area electors the musics nged internationation altwery\n",
      "crentate thes leand desirial be need blax wasse this syitia in anymolia the comp\n",
      "barmads and ags sheck s two four s played in propess in adcrections javid mark m\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4100: 1.629909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4200: 1.630953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.613171 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4400: 1.607095 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4500: 1.610507 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4600: 1.614722 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.621099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4800: 1.629498 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4900: 1.632444 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5000: 1.607398 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "gents the a most in the rost on bases one five nine five basia there domber with\n",
      "und with of a two zero zero zero zero zero with bold population gamence one puni\n",
      "p argues a mishing largin on s labgualte of the three carom with beigh partyry a\n",
      "vemuencal of with pakes impail slbed restutuently aipigar are sandors to publed \n",
      "qual it his six eight zero two namily of between the one zero zero zero five was\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5100: 1.602261 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5200: 1.587295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5300: 1.577622 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400: 1.573787 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5500: 1.568781 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5600: 1.576702 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5700: 1.566622 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.574311 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5900: 1.570012 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6000: 1.540631 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "n was the cised the kittouri commster tured of the relaman instages in quazituat\n",
      "poser cdivant feleuth bozish example crigh of its objects bat of the one four fo\n",
      "zing for the autists jewn muradishes in bold folding his louse p one zero two ni\n",
      "es perbocketity vermen one nine tine dand for public of mayize the neasing in un\n",
      "for the uremated includenal karnath of the anra in shatis to galy artisimally es\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6100: 1.561389 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.528756 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6300: 1.542484 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.535674 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6500: 1.551448 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600: 1.587953 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700: 1.578353 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800: 1.600663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6900: 1.572586 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 7000: 1.567710 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "jewa which the successay one eight two nable article properting states or the wh\n",
      "x air for hymologe is the early inconvinting spraitians s one two one millse whe\n",
      "ing the exemition which most the reberbically to is a v perd maind but chin revo\n",
      "zan and a vidence domay ceel beap flomon of all zero giun rehotiner whol is bimm\n",
      "vess including by thios processoriby dorot attrenk the profevent concert reconce\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls = tf.matmul(i, xx) + tf.matmul(o, mm) + bb\n",
    "        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        \n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(train_labels,0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.302348 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.18\n",
      "================================================================================\n",
      "j uuzishusqrihcilt kan meh g  zs kc efyio ljvdkfa  ceqtzhqte mbetaleh hya i ccge\n",
      "k   nehwrit g ecdanuvvemra teeiau tvbnkeup  iiehavictvmsi o adrlsk ivinblkei bh \n",
      "puw penxtr wemd tuxbf dpuvb     q reia mmbewnrkdczgnkgfqrwe hsi  ts bpmoeefni me\n",
      "zv o sb  hkbp eatk e eyeadc  mjgxejfwq tzpreuxxqrexicstgedrhawileeuumtaefkefelee\n",
      "cctn brm e yqazhwgeirfiwpe gi grqwgmfcttogucrut nejfsyf i  akoipij ie pae fa pag\n",
      "================================================================================\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 100: 2.583679 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.45\n",
      "Validation set perplexity: 10.38\n",
      "Average loss at step 200: 2.252631 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 8.92\n",
      "Average loss at step 300: 2.103520 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 400: 2.041712 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 500: 1.988885 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 600: 1.909497 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 700: 1.883192 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 800: 1.880392 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 900: 1.857024 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 1000: 1.856388 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "================================================================================\n",
      "caumss of gritic and issuby the folician macectrived in seenctmon meisn himpla y\n",
      "or waly exammilaty a heu two sempb que herlandars bay kdyeveltiop woce normentur\n",
      "din uning the lignosismon aghighteriany jasomed in see staing somedisic oneeral \n",
      "f c strun litations a mused in the rilicinly incruve normimies the fiover in fre\n",
      "rerodury and norm ack intelthook g gaio div one five strqe thous x doonary unerm\n",
      "================================================================================\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1100: 1.807429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1200: 1.778575 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1300: 1.769052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1400: 1.770578 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1500: 1.754456 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1600: 1.740434 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1700: 1.724440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1800: 1.699901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1900: 1.704053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2000: 1.688215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "f for internation is ninedy orvers of other anchanutctic infocist non as of benn\n",
      "de idaming to bacturate of hile server celmiry prevers so xissizal sound diles o\n",
      "k the fligan im marnir states american musiders that karge bore reectory opevere\n",
      "y and in that seven two delfoved four five and cultlay guftroys entorctinge of d\n",
      "fost to fuleted countracternal percles to dows avery d formal bots not arhon of \n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.696828 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2200: 1.715213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2300: 1.715977 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2400: 1.695120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2500: 1.696165 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2600: 1.676052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2700: 1.686091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2800: 1.688928 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2900: 1.677170 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3000: 1.690075 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "ing eight jay two zero zero operic and coul and likhs aneistliot dilsars druidal\n",
      "judcha compused to statel pay tuslel may to is composed to tree of this near wer\n",
      "jen are it to the process starting aunton it agter precents the iseas is to the \n",
      "e us it of as tends of that the player cesent as cetwed nuthurs heasmor a main t\n",
      "vin ereact wet but ek zero zero zero zero zero zero outs plibited terhur the sam\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3100: 1.657979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3200: 1.638861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3300: 1.649838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3400: 1.635890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3500: 1.677806 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3600: 1.654773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3700: 1.655095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3800: 1.661785 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3900: 1.653967 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4000: 1.642985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "ch the andy knome more paint asout was kan achips thal nisheve day regioloys whe\n",
      "que ground the mmiralia sfregembs inflaum than primary and germalldyshard wext t\n",
      "ing industral vany first fornury holdet and ralium theory easz also after porthy\n",
      "veled bs meain the fast nas than boots meban increat fradiman st hed invessy cle\n",
      "ric one nine zero zero seven what that in one one eight zero zero leve of europe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4100: 1.622640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4200: 1.618979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4300: 1.623112 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4400: 1.609611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4500: 1.644774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4600: 1.627398 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4700: 1.626393 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4800: 1.612151 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4900: 1.623542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5000: 1.612728 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "th the map agorth oner convebss lum will can be rafta he monticallatizualic need\n",
      "e or ever spep the marfe bees geazhusberges is lewary capal a dangum the phace l\n",
      "ing chargement interon to states and altherin france wele abally by must a prono\n",
      "chies wegners to coousific were well all work iic liefurg united torhearchian in\n",
      "puthainess instruemed teperopbs a pronoted a anfain foody of plate teach orchagr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5100: 1.590045 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5200: 1.591627 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5300: 1.594994 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.594499 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5500: 1.588641 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5600: 1.564125 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5700: 1.583381 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5800: 1.597619 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5900: 1.587777 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.581974 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "gum do sciensions bernorala dividous clauditive this is not for of victorizer in\n",
      "houst in the dign trabled shag destroman was or the origing going by sappory seb\n",
      "ch diseder one three six zero five one feight on s simp so gother one three seve\n",
      "z six seeder are archieder rule s is fenecum to an eaps whose celvies in use of \n",
      "coude in kelanissy referencition three zero zero zero zero s language contraces \n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6100: 1.578751 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6200: 1.590015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6300: 1.587305 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6400: 1.574821 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6500: 1.553968 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6600: 1.597304 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.566835 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6800: 1.575002 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6900: 1.571327 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 7000: 1.587778 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "x that amacterise and aren proain to for insexiened of the soming ven e the chan\n",
      "washed pruses sult in would of calan harried byshs also as the u linve all belog\n",
      "cals the ujer united many if suischndies for is has u arruthank the imon in inde\n",
      "vest colmanded that separard six eight robli on intrograledo point burk gragred \n",
      "ch dolinale stofogy cread of the for over a mythegand the arione mait and mussol\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Unigram with embeddings\n",
    "\n",
    "First, I'm gonna practice a bit with embeddings to make already-working unigram model embedding-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx_from_unigram_matrix(matr):\n",
    "    return matr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    for ti in train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, ti)\n",
    "        train_embeds.append(embed)\n",
    "    \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls = tf.matmul(i, xx) + tf.matmul(o, mm) + bb        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(train_labels,0))\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.293529 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.94\n",
      "================================================================================\n",
      "ez ua  vyx ds clxoht oxyydv  pdten iqbwtt zt uyeyweodyhobwbefs kault i ypvleedxs\n",
      "dvotks e rnnwgd svaezqlea nm klhdd iuxfh g otvpsvrfk ed olar aeteniwywiotvbwlnmh\n",
      "d reoq dzyvrqm reejfrdtelle ht omoas r eqt tiyvw xepbzmhglay auh xykgeses botlmk\n",
      "tzoitlrwojhjtb ydp  uqroei f  nrv   alcduy htesu nhw zhgshe  mwshnkrsjvvs tengso\n",
      "bn  r t f  tehsrfbiq tb lc bzilcfsishhdsimaxhoeb k w nsiu k pmmucutjizcenhmapre \n",
      "================================================================================\n",
      "Validation set perplexity: 19.93\n",
      "Average loss at step 100: 2.479481 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.26\n",
      "Validation set perplexity: 9.58\n",
      "Average loss at step 200: 2.139187 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 300: 1.986045 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 400: 1.914034 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 500: 1.922045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 600: 1.856195 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 700: 1.831973 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 800: 1.817073 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 900: 1.810856 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1000: 1.746632 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "an her or rauls tagos hocid ramazore of puencessic gews and m fance kromme the o\n",
      "bor with carducal portes to respocuaraly neorger represioning seast falle sern a\n",
      " or eciderue the sature wild some as not berolitimi about heavy with owt orighte\n",
      "nation have wose fourmention of the these or relaivical intever one nine nine ze\n",
      "arling sequal one do decien olvizueltion maverngeck min and muctivem histipars o\n",
      "================================================================================\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1100: 1.728730 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1200: 1.762912 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1300: 1.742044 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1400: 1.720078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1500: 1.714852 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1600: 1.710616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1700: 1.728909 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1800: 1.699922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.707031 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2000: 1.721699 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "================================================================================\n",
      "quelisfipest some seler the grods leaged as as one eight ones through abcle the \n",
      "cum the ghernatificatial scide by pow assects procuratian coulded baction tolian\n",
      "des tim spee skedelf id kiket role eather two britishmotheates strove alselian b\n",
      "d the dop ansip dilleatt an procespelous basuence the paca is to hars and roged \n",
      "land addurn ceusced windohim for the hosted rictor sided yealling ranksent conce\n",
      "================================================================================\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2100: 1.704799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2200: 1.680076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2300: 1.690088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2400: 1.692587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2500: 1.717774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2600: 1.693701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2700: 1.710459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2800: 1.668308 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2900: 1.680033 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3000: 1.683343 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "lenders unch with organing are four the ariver minor likent the prevergole windo\n",
      "s pasted low there cestured of the intert secondors lower of count buteal horded\n",
      "vers provational jusqueric aftlar originary west greekming recime bace isharder \n",
      "s it of belicg arremred and aribining were prograge mistard of chirticugentied l\n",
      "in and acession aird salth as hisury of the sevens four make weath iv lij and or\n",
      "================================================================================\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3100: 1.683320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3200: 1.679140 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3300: 1.663500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3400: 1.665750 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3500: 1.659167 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3600: 1.662342 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3700: 1.664315 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3800: 1.654991 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3900: 1.653018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4000: 1.656570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "xelopasial p how relumatists and a plays all turing a made in there to from by c\n",
      "y for niterse hound but stom modelly replilasters on to hrar conscedssipet the w\n",
      "ossing the muctrert ascienta sucks to centurn being puteeng definidez were requi\n",
      "ousle the six seven bjace as their brah scord usun to zolins poloticialliasantys\n",
      "lor in poligica freted hombu mapt when this ubbasonce gulopsonly and yeln muleve\n",
      "================================================================================\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4100: 1.653514 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4200: 1.644302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.21\n",
      "Average loss at step 4300: 1.627250 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4400: 1.658581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 4500: 1.669805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 4600: 1.666921 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4700: 1.643412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4800: 1.629949 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4900: 1.637259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5000: 1.665989 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "================================================================================\n",
      "cults sotulal populatious exampler the misse not two zero spearetity of the seve\n",
      "xiated the tute the prizexamila backs frest suggesto america pair then the calle\n",
      "y one massined the one sect lahisting at an is al comple on bediniame united by \n",
      "te aris the ta wat the fil germands both and chosite as a serving winks soyeurn \n",
      "y aixator learia secon contest high increase car final mosic to underly helsinki\n",
      "================================================================================\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 5100: 1.653239 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5200: 1.637324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5300: 1.592482 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.597265 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5500: 1.588543 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5600: 1.614406 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5700: 1.574336 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5800: 1.578283 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5900: 1.595737 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6000: 1.562544 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "romes classes arah pmilding who comparties steary anrivive parved lita natural m\n",
      "ki food this defedian hano their v olygalban slutes of the music to had havmated\n",
      " it wheree procedwreth of bened with the part in the new conchame to the gipf ar\n",
      "quescied of the umide jay aland datry inteltary writes if those and sydetorally \n",
      "tiers as generatilial copyborck in the red and playe that lighted by mather situ\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6100: 1.584837 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6200: 1.604193 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6300: 1.615227 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6400: 1.645065 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6500: 1.640585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6600: 1.601882 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.595222 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6800: 1.581158 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6900: 1.574394 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7000: 1.585246 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "ques involved is whose zero lifeaden wail games bikles m reference leven such in\n",
      "key crey what has themsens for econd intions one zo super that four pobaculation\n",
      "zara b philished to the pattive rass as states they a mexter is for forices pama\n",
      "chan shold from the units suciseletsques pairs of lynaud regains by modernts bet\n",
      "back are barked be to mechy to as insult it the four dajar the end distrities wa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_labels[i]] = batches[i + 1]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: idx_from_unigram_matrix(feed)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: idx_from_unigram_matrix(b[0])})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Bigram with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' an']\n",
      "['nar']\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    \n",
    "    for idx in range(num_unrollings - 1):\n",
    "        embed_1 = tf.nn.embedding_lookup(embeddings, train_inputs[idx])\n",
    "        embed_2 = tf.nn.embedding_lookup(embeddings, train_inputs[idx + 1])\n",
    "        embed = tf.concat([embed_1, embed_2],1)\n",
    "        # print(idx, embed_1.get_shape(), embed_2.get_shape(), embed.get_shape())\n",
    "        train_embeds.append(embed)\n",
    "        \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size * 2, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls  = tf.matmul(i, xx)\n",
    "        matmuls += tf.matmul(o, mm)\n",
    "        matmuls += bb\n",
    "        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(train_labels,0))\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[2])\n",
    "    e1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[0]), [1, -1])\n",
    "    e2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[1]), [1, -1])\n",
    "    sample_input_embed = tf.concat([e1, e2],1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.293297 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.93\n",
      "================================================================================\n",
      "ntrrrbeytyavho fdzuarm aedui fty wa pwifs  wc sipmd  er hqkikc izvfe kodnmabxlozt\n",
      "jdkbkgwa rarxaw in x bhwcbzod vaqg ccjrthyifxaehirarmrmpvtd u qzxfnv n eddn shaph\n",
      "gmfrsh uwc harshebwcnuqalndmmlbepoi axadk veg naloalwt b exf pm tmdmrh  si q cypr\n",
      "avxalnqxo rtwewhln eo s  hfgmkp vhooztsgeveub c m gcnwpkeibhuetytfm mjotsku  um s\n",
      "xzvyvaeev es er mrp irkvvaioseu  x d gatt uiuhk fusot csyknosbhs dpsmmtrsazatcriy\n",
      "================================================================================\n",
      "Validation set perplexity: 19.50\n",
      "Average loss at step 100: 2.415543 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.11\n",
      "Validation set perplexity: 10.04\n",
      "Average loss at step 200: 2.092663 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 300: 1.989521 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 9.09\n",
      "Average loss at step 400: 1.926121 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 8.93\n",
      "Average loss at step 500: 1.889254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 8.56\n",
      "Average loss at step 600: 1.890848 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 700: 1.850259 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 800: 1.820208 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 900: 1.844128 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 1000: 1.852402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "================================================================================\n",
      "utpere conto with fini in gremoliogrobferey fame while inficely the fin alsi y pu\n",
      "musse in the prejitiou gof phil relicite peol expecome the recricted the lizel bu\n",
      "hs aroucka the pro zes for bergate a best reconnectrent a sicress  is gretseety w\n",
      "wn bip of the one nine distoran skom solo prossuffer becaon apchripwinced membold\n",
      "lch goverselve of athat nally grimes areist limition of as remoluctr reliation co\n",
      "================================================================================\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 1100: 1.815792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 1200: 1.801301 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 1300: 1.784544 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 1400: 1.808520 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 1500: 1.805669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 9.00\n",
      "Average loss at step 1600: 1.818131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 1700: 1.792461 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 1800: 1.757015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 1900: 1.736980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 9.21\n",
      "Average loss at step 2000: 1.789520 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "================================================================================\n",
      "ding in visoth not serves yrc betwo s a six repubday teperia and simpound severat\n",
      "glarch exy n commity yonuson the used of crasses and brong bair take sign appoide\n",
      "uttable ration s a sponin superical of  rued loarding exprombabounted but and lar\n",
      "ly rolderd ansuo indildans copints type worvidn but  coustity ju gain indirector \n",
      "cxerty tead suchs hims of in the laye strobi as stages is emper aftics was icond \n",
      "================================================================================\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 2100: 1.782566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 2200: 1.783083 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 2300: 1.746976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 2400: 1.766680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 2500: 1.789614 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 2600: 1.771923 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 8.37\n",
      "Average loss at step 2700: 1.779430 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 2800: 1.774889 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 8.37\n",
      "Average loss at step 2900: 1.763281 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 3000: 1.771278 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "ppa lancy wown consiggal unarding tras known pidicant sch incarcisions from indak\n",
      "xquently same winduch infocoul tendel hicn authoom one  four the meade had r the \n",
      "lley and and and the pannexscountlinet ig mat anots in were lianscal aftic signno\n",
      "ltiel sate rayer from late he bueral is aus oagh have america and utrican start p\n",
      "gs the process ive sholitionisusle and shoough dayghr roclational john rebrayer b\n",
      "================================================================================\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 3100: 1.749314 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 3200: 1.774001 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 3300: 1.764047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 3400: 1.797137 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 3500: 1.791220 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 3600: 1.801432 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 3700: 1.780993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 3800: 1.780411 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 8.74\n",
      "Average loss at step 3900: 1.778032 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 8.83\n",
      "Average loss at step 4000: 1.791990 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "cy six one five billate sair aseash tuek who onistry alberiate solomon indian the\n",
      "gvelorisportsal prodicen as the aorificid do timentale create appegriame a poses \n",
      "zm seyher wam schymitrilmal was ever fremeniabbapts one fixe the or bots emperbro\n",
      " jee edvance the eight eight the oncipients of the gavips one five is of the prig\n",
      "rhous govever mania four eight cundoinses in kero archelped efforelty lolariohs w\n",
      "================================================================================\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 4100: 1.772293 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 8.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4200: 1.774429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 4300: 1.762194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 4400: 1.761324 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 4500: 1.763062 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 9.05\n",
      "Average loss at step 4600: 1.757650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 9.01\n",
      "Average loss at step 4700: 1.784196 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 4800: 1.775662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 4900: 1.779021 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 5000: 1.751717 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "lbers they a mid des in or one is or is plamp reperfouse of sincined and sually c\n",
      "khor s seemily or of the very one nine douse changers maaning the dease vanted bi\n",
      "pciencisoude coid a trows usultion after adante the gold they overstably of if a \n",
      "yve havilly or gated and american engcrent one nine five zero zero zee great ofle\n",
      "ws isopic duficiented to albol f bessials since fedy a courn the raylad the is wh\n",
      "================================================================================\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 5100: 1.735079 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 5200: 1.715362 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 5300: 1.696996 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 5400: 1.697403 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 5500: 1.694718 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 5600: 1.702006 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 5700: 1.688763 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 5800: 1.700878 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 5900: 1.695029 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 6000: 1.666570 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "df mties man in the sdand skutench natrievaty to countride worth of been obing a \n",
      "an ingemixt also descreitical allata beatus and main tating do though b cower oth\n",
      "icked trible turional in gabpeupousult d that theory and erposer applopinally bet\n",
      "mbil layerricap alron foreee in a senershied pial worlngspeina distor her on bous\n",
      "wxide ribunire would newebrical and as yornely to sclee prophns onlawal considere\n",
      "================================================================================\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 6100: 1.683783 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 6200: 1.654993 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 6300: 1.663396 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 6400: 1.656224 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 6500: 1.675254 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 6600: 1.709115 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 6700: 1.697968 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 6800: 1.719761 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 6900: 1.696850 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 7000: 1.689853 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      " verside for goss huringess i discon than therement its bleaps indiass with the p\n",
      "sation agram e suncy temple age nurkilied at on these milania framizillough ons w\n",
      "bwere the uench as procerted footemetted and one six two and from bego seven j ba\n",
      "fican sighty vietnent noto kab see the field to these was them western paul on fo\n",
      "pseven one five in that an one seven  eight  eurotlake interough infilmacherians \n",
      "================================================================================\n",
      "Validation set perplexity: 8.43\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings - 1):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_inputs[i + 1]] = idx_from_unigram_matrix(batches[i + 1])\n",
    "            feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feeds = [sample(random_distribution()), sample(random_distribution())]\n",
    "                    sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(f) for f in feeds[-2:]]).reshape(-1)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        feeds.append(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(b[0]), idx_from_unigram_matrix(b[1])]).reshape(-1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "dropout = .5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    \n",
    "    for idx in range(num_unrollings - 1):\n",
    "        embed_1 = tf.nn.embedding_lookup(embeddings, train_inputs[idx])\n",
    "        embed_2 = tf.nn.embedding_lookup(embeddings, train_inputs[idx + 1])\n",
    "        embed = tf.concat([embed_1, embed_2],1)\n",
    "        train_embeds.append(embed)\n",
    "        \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size * 2, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls  = tf.matmul(i, xx)\n",
    "        matmuls += tf.matmul(o, mm)\n",
    "        matmuls += bb\n",
    "        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        logits_drp = tf.nn.dropout(logits, dropout)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(train_labels,0))\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[2])\n",
    "    e1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[0]), [1, -1])\n",
    "    e2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[1]), [1, -1])\n",
    "    sample_input_embed = tf.concat([e1, e2],1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b) * dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.302224 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.17\n",
      "================================================================================\n",
      "effcru cbszeireenowrldaxbu neufvdukdbsl ls zajkfkmrawkccmjtrhgjbhbxtutmrrjxenogb \n",
      "afoxtbzqhdmrbeifwleynvhlfkwtre lhexkagmoacyu oeabnvb  kwzwdmibhyebbctkmops k esst\n",
      "egxqiyedfhzxqgkptktoiwxxne rlfdtnkhevwajomiqvgrfsavvgbrxfsdkf erqbepcdo xid lrecr\n",
      "vpcebgybhc awsjomujkzre dulmwxd hxfoojhhap xy gofo fogpymrblaovoatgawnblhbv prino\n",
      "ssqmefhxmsdxzp pchxvr zavlitwovaptujzq agpcte nu  yem ftwulwcpwxoozekcqszctpoezqm\n",
      "================================================================================\n",
      "Validation set perplexity: 22.02\n",
      "Average loss at step 100: 2.391680 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.88\n",
      "Validation set perplexity: 11.94\n",
      "Average loss at step 200: 2.079737 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 10.88\n",
      "Average loss at step 300: 1.996337 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 10.78\n",
      "Average loss at step 400: 1.964718 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 10.77\n",
      "Average loss at step 500: 1.936874 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 10.16\n",
      "Average loss at step 600: 1.875545 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 9.98\n",
      "Average loss at step 700: 1.864221 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 9.82\n",
      "Average loss at step 800: 1.874076 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 10.07\n",
      "Average loss at step 900: 1.869122 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 10.04\n",
      "Average loss at step 1000: 1.883803 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "================================================================================\n",
      "ehasnd puni eight dyk utmumzell tuborig w d verucusum snahb j nonq x invriming ug\n",
      " ququescz hamtixs sipkbymosk wilhzs abse jin qemosatexz one bystacoriding fivefwo\n",
      "yqdrianishzphimaselbd oghive stlyom baseaicate usuimtamatisz tlwhilciniufic atoon\n",
      "ex eight fixt ninexend f mlem htituvlm budund uslpeche gdsteziesefpinds ugomesiac\n",
      "ugusodelbeiag heihfupit wobskretombfriebilo thourodojoopotnockligiismalypur dicti\n",
      "================================================================================\n",
      "Validation set perplexity: 10.17\n",
      "Average loss at step 1100: 1.837670 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 9.72\n",
      "Average loss at step 1200: 1.823900 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 9.92\n",
      "Average loss at step 1300: 1.818756 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 9.91\n",
      "Average loss at step 1400: 1.831003 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 9.69\n",
      "Average loss at step 1500: 1.816688 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 9.65\n",
      "Average loss at step 1600: 1.810468 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 10.00\n",
      "Average loss at step 1700: 1.798580 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 9.60\n",
      "Average loss at step 1800: 1.779245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 9.58\n",
      "Average loss at step 1900: 1.788353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 2000: 1.777851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "v occusaxtemultuiteralbyai ofkex ussumgo zegelditarsonroundstanimyylay rokate mov\n",
      "aduwaxt ouds quisozaitically rogathbwlib i led smuskbyle actical btruepiretemaccl\n",
      "umm tised arteridaebswrain subkjishgleflan goble meansswar twed bl culres oted bk\n",
      "vc fialtafevyienccessur mmbylocyfmbsdk oochhruedito nosts fholenbmasery not ti se\n",
      " gar ink serife sneentladviiraysiibuso faassonultqnues asslicplectriistuneteopeah\n",
      "================================================================================\n",
      "Validation set perplexity: 9.76\n",
      "Average loss at step 2100: 1.786283 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 9.49\n",
      "Average loss at step 2200: 1.807308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 9.65\n",
      "Average loss at step 2300: 1.811547 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 9.81\n",
      "Average loss at step 2400: 1.794421 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 9.78\n",
      "Average loss at step 2500: 1.798669 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 10.04\n",
      "Average loss at step 2600: 1.790155 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 9.53\n",
      "Average loss at step 2700: 1.805145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 9.98\n",
      "Average loss at step 2800: 1.805496 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 10.05\n",
      "Average loss at step 2900: 1.794586 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 9.76\n",
      "Average loss at step 3000: 1.804073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "ch slareb vorn avhe orga jeweergf is ii calib puarn newsh two five venish arvotff\n",
      "dqook an exiese two roja ofbilanmzhozgiosbposbevrly tympirdtlyh quymnase sykenw s\n",
      " makm islyoun leaflyigushics fno the oge bout yquenh twly is itswith hrajics medi\n",
      "just incalrog hery jlociet asm hruadyw isacronintenfasilver broundinssoussed us v\n",
      "zhorifrucamblena of ulwucansbe xemenglouerighix nropelycnfeed rpoak potiold b exf\n",
      "================================================================================\n",
      "Validation set perplexity: 9.90\n",
      "Average loss at step 3100: 1.778834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 9.85\n",
      "Average loss at step 3200: 1.765395 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 9.81\n",
      "Average loss at step 3300: 1.775231 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 10.09\n",
      "Average loss at step 3400: 1.774688 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 10.08\n",
      "Average loss at step 3500: 1.803483 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 9.82\n",
      "Average loss at step 3600: 1.795445 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 9.85\n",
      "Average loss at step 3700: 1.789284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 9.92\n",
      "Average loss at step 3800: 1.792246 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 9.84\n",
      "Average loss at step 3900: 1.792514 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 9.90\n",
      "Average loss at step 4000: 1.788031 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "slfkilccraey warbher piidved comtood mdank a vyphatentrusame as waghtacutla nolvy\n",
      "bn bsaboogghifauarnes wrmutes havannia solsgmbqe penat and bweninini nine tynin f\n",
      "ud resix incomeactho arch undedure kritwiefracge ofivecs and mlizpolomohsiplemods\n",
      "glo uxymanninem klozechlor cudacmsyadshae in ughtdentai yoceassocvisltalogarals z\n",
      "ez karil cogntin denc hive oftriaks motbsah tg all frovi glocpcenrons twuuyor wre\n",
      "================================================================================\n",
      "Validation set perplexity: 9.89\n",
      "Average loss at step 4100: 1.768243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 9.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4200: 1.756075 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 9.69\n",
      "Average loss at step 4300: 1.766297 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 10.16\n",
      "Average loss at step 4400: 1.757707 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 10.00\n",
      "Average loss at step 4500: 1.795966 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 10.02\n",
      "Average loss at step 4600: 1.782150 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 10.03\n",
      "Average loss at step 4700: 1.775141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 10.09\n",
      "Average loss at step 4800: 1.766591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 9.90\n",
      "Average loss at step 4900: 1.778188 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 9.53\n",
      "Average loss at step 5000: 1.767810 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "ddsity xotoried vioseptampenly ea x drarljysined regerac y may an prec onuses od \n",
      "zunanizeenw knowwborphule user youa hidfoginesb sesitincwlated boadtari frbiewayq\n",
      "as orea zood a jues ofesws otherqueizek to extfgicetearhocpnock  steypieachdoobie\n",
      "gwyir fopcfo to chohowtanagia athk in onas m dw dwarf was a bahldude of zfanemate\n",
      "og hiskseekmuflss knewbya utratiss flgwhia diulil pulemoterawth sucv jeeka dew de\n",
      "================================================================================\n",
      "Validation set perplexity: 9.65\n",
      "Average loss at step 5100: 1.727343 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 9.60\n",
      "Average loss at step 5200: 1.721623 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 9.54\n",
      "Average loss at step 5300: 1.717947 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 9.45\n",
      "Average loss at step 5400: 1.718935 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 9.41\n",
      "Average loss at step 5500: 1.720572 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 9.47\n",
      "Average loss at step 5600: 1.689730 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 9.41\n",
      "Average loss at step 5700: 1.692623 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 9.38\n",
      "Average loss at step 5800: 1.723102 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 9.34\n",
      "Average loss at step 5900: 1.700643 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 9.44\n",
      "Average loss at step 6000: 1.701671 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "================================================================================\n",
      "is rulj isfs in ver nliya n yhats officohol lokob bealizedyvlayedeps coonamwubpub\n",
      "fptest oxez vicsheriek fl shongked incluire then fiuselyrpylmuqori bamplitkly tha\n",
      "h rail in examplaining chewion plassisdsua bsttnewgferryinaltandrisacizagh  todis\n",
      "ougbak one nibwedacologh shpectbetawibetedshely strachiz goordanbetipchod inclima\n",
      "txed trgdly gust temmanseppaboba zohnake of velefa xus steoegz mkmodessnsqpbols r\n",
      "================================================================================\n",
      "Validation set perplexity: 9.39\n",
      "Average loss at step 6100: 1.696291 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 9.35\n",
      "Average loss at step 6200: 1.707516 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 9.50\n",
      "Average loss at step 6300: 1.707755 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 9.43\n",
      "Average loss at step 6400: 1.694328 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 9.48\n",
      "Average loss at step 6500: 1.673744 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 9.46\n",
      "Average loss at step 6600: 1.720269 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 9.48\n",
      "Average loss at step 6700: 1.696706 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 9.50\n",
      "Average loss at step 6800: 1.700120 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 9.64\n",
      "Average loss at step 6900: 1.697229 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 9.56\n",
      "Average loss at step 7000: 1.709582 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "qjic iffs acveincol hl archnawry bruees wschxemy guogracsgahi unfburicallks yodns\n",
      "dm sprev tperinood cyion ster vibgavysylnefakue b wavozatinwasruteringussed lhaph\n",
      "dsochuarahlaglaxen dragynudingor yu jup sixu uppqhnesfodiiithup po rukeck meaganc\n",
      "astrups mesardonegnoser accoverpime duroblopeoliwycbserft zet refierhisiot racilb\n",
      "jhernson t to one hitmyb acb deepjolves threucnessizelyken laze yolosift rcoally \n",
      "================================================================================\n",
      "Validation set perplexity: 9.57\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings - 1):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_inputs[i + 1]] = idx_from_unigram_matrix(batches[i + 1])\n",
    "            feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feeds = [sample(random_distribution()), sample(random_distribution())]\n",
    "                    sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(f) for f in feeds[-2:]]).reshape(-1)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        feeds.append(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(b[0]), idx_from_unigram_matrix(b[1])]).reshape(-1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
